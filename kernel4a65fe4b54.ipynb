{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scotty = pd.read_csv(\"/kaggle/input/scotty/data/data-train.csv\")\nscotty.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"scotty = scotty[['start_time','src_sub_area']]\n#scotty.head()\n\nscotty.info()\n\nscotty.start_time = scotty.start_time.astype('datetime64[ns]')\nscotty.columns = ['datetime','src_sub_area']\n# print(scotty.head())\n# print(scotty.tail())\n\nscotty['datetime'] = scotty['datetime'].apply(lambda x: x.replace(minute=0, second=0))\n# print(scotty.head())\n# print(scotty.tail())\n\nscotty_group = scotty.groupby(['datetime','src_sub_area']).count()\nscotty_group.reset_index(inplace=True)\n\na = scotty.groupby(['datetime','src_sub_area'])['src_sub_area'].count()\nscotty_group['demand'] = a.reset_index(drop=True)\n# scotty_group.head()\n\nscotty_group.sort_values(by=['src_sub_area','datetime'], inplace=True)\n\nscotty_group = scotty_group.groupby('src_sub_area').\\\n    apply(lambda x : x.set_index('datetime').resample('H').sum().fillna(0)).\\\n    reset_index()\n\nsxk97 = scotty_group[scotty_group.src_sub_area == 'sxk97']\nsxk9e = scotty_group[scotty_group.src_sub_area == 'sxk9e']\nsxk9s = scotty_group[scotty_group.src_sub_area == 'sxk9s']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sxk97 = sxk97.drop(columns=['src_sub_area'])\nsxk97.set_index('datetime', inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df = sxk97\n# df.shape\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## From MachineLearningMastery"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\n\nscaler.fit(train_data)\nscaled_train_data = scaler.transform(train_data)\nscaled_test_data = scaler.transform(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaled_train_data.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# univariate multi-step vector-output stacked lstm example\nfrom numpy import array\nfrom keras.models import Sequential\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\n\n# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps_in, n_steps_out):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        # check if we are beyond the sequence\n        if out_end_ix > len(sequence):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)\n\n# define input sequence\n# raw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\nraw_seq = train_data['demand'].tolist()\n# choose a number of time steps\nn_steps_in, n_steps_out = 24*7, 2\n# n_steps_in, n_steps_out = n_test*3, n_test\n# split into samples\nX, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n# reshape from [samples, timesteps] into [samples, timesteps, features]\nn_features = 1\nX = X.reshape((X.shape[0], X.shape[1], n_features))\n# X = raw.reshape(raw.shape[1], raw.shape[0], n_features)\n# y = test_data.values\n# y = y.reshape(y.shape[1], y.shape[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define model\nmodel = Sequential()\nmodel.add(LSTM(100, activation='relu', return_sequences=True, input_shape=(n_steps_in, n_features)))\nmodel.add(LSTM(100, activation='relu'))\nmodel.add(Dense(n_steps_out))\nmodel.compile(optimizer='adam', loss='mae')\n# fit model\nmodel.fit(X, y, epochs=2, verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# model.summary()\nprint(X.shape)\nprint(y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losse = model.history.history['loss']\nplt.plot(range(len(losse)), losse)\nplt.show()\n# losse","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# demonstrate prediction\nx_input = array([70, 80, 90])\nx_input = x_input.reshape((1, n_steps_in, n_features))\nyhat = model.predict(x_input, verbose=0)\nprint(yhat)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"[[120.097084 138.88902 ]]\n[[116.9725  131.78836]]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# split a univariate sequence into samples\ndef split_sequence(sequence, n_steps_in, n_steps_out):\n    X, y = list(), list()\n    for i in range(len(sequence)):\n        # find the end of this pattern\n        end_ix = i + n_steps_in\n        out_end_ix = end_ix + n_steps_out\n        # check if we are beyond the sequence\n        if out_end_ix > len(sequence):\n            break\n        # gather input and output parts of the pattern\n        seq_x, seq_y = sequence[i:end_ix], sequence[end_ix:out_end_ix]\n        X.append(seq_x)\n        y.append(seq_y)\n    return array(X), array(y)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"n_test = 24*7\ntrain_data = df[:len(df)-n_test]\ntest_data = df[len(df)-n_test:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# define input sequence\nraw_seq = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n# choose a number of time steps\nn_steps_in, n_steps_out = 3, 2\n# split into samples\nX, y = split_sequence(raw_seq, n_steps_in, n_steps_out)\n# summarize the data\nfor i in range(len(X)):\n\tprint(X[i], y[i])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## From TowardsDataScience"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = df[:len(df)-252]\ntest_data = df[len(df)-252:]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler.fit(train_data)\nscaled_train_data = scaler.transform(train_data)\nscaled_test_data = scaler.transform(test_data)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.preprocessing.sequence import TimeseriesGenerator\n\nn_input = 1000\nn_features= 1\ngenerator = TimeseriesGenerator(scaled_train_data, scaled_train_data, length=n_input, batch_size=252)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\n\nlstm_model = Sequential()\nlstm_model.add(LSTM(10, activation='tanh', input_shape=(n_input, n_features)))\nlstm_model.add(Dropout(0.2))\nlstm_model.add(Dense(1))\nlstm_model.compile(optimizer='adam', loss='mse')\n\nlstm_model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_model.fit_generator(generator,epochs=50)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses_lstm = lstm_model.history.history['loss']\nplt.figure(figsize=(12,4))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.xticks(np.arange(0,21,1))\nplt.plot(range(len(losses_lstm)),losses_lstm);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_predictions_scaled = list()\n\nbatch = scaled_train_data[-n_input:]\ncurrent_batch = batch.reshape((1, n_input, n_features))\n\nfor i in range(len(test_data)):   \n    lstm_pred = lstm_model.predict(current_batch)[0]\n    lstm_predictions_scaled.append(lstm_pred) \n    current_batch = np.append(current_batch[:,1:,:],[[lstm_pred]],axis=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"aa = lstm_model.predict(scaled_train_data.reshape(1, 1260, n_features))\n# aa = scaler.inverse_transform(aa)\n# plt.plot(train_data.index, aa)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_predictions_scaled","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_predictions = scaler.inverse_transform(lstm_predictions_scaled)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"lstm_predictions","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data['LSTM pred'] = lstm_predictions\ntest_data.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plt.plot(test_data.index, test_data['demand'])\nplt.plot(test_data.index, test_data['LSTM pred'])\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Based on TF web"},{"metadata":{"trusted":true},"cell_type":"code","source":"import tensorflow as tf","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def univariate_data(dataset, start_index, end_index, history_size, target_size):\n  data = []\n  labels = []\n\n  start_index = start_index + history_size\n  if end_index is None:\n    end_index = len(dataset) - target_size\n\n  for i in range(start_index, end_index):\n    indices = range(i-history_size, i)\n    # Reshape data from (history_size,) to (history_size, 1)\n    data.append(np.reshape(dataset[indices], (history_size, 1)))\n    labels.append(dataset[i+target_size])\n  return np.array(data), np.array(labels)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uni_data = sxk97.values\nTRAIN_SPLIT = 1000","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uni_train_mean = uni_data[:TRAIN_SPLIT].mean()\nuni_train_std = uni_data[:TRAIN_SPLIT].std()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"uni_data = (uni_data-uni_train_mean)/uni_train_std","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"univariate_past_history = 20\nunivariate_future_target = 0\n\nx_train_uni, y_train_uni = univariate_data(uni_data, 0, TRAIN_SPLIT,\n                                           univariate_past_history,\n                                           univariate_future_target)\nx_val_uni, y_val_uni = univariate_data(uni_data, TRAIN_SPLIT, None,\n                                       univariate_past_history,\n                                       univariate_future_target)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"BATCH_SIZE = 256\nBUFFER_SIZE = 10000\n\ntrain_univariate = tf.data.Dataset.from_tensor_slices((x_train_uni, y_train_uni))\ntrain_univariate = train_univariate.cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n\nval_univariate = tf.data.Dataset.from_tensor_slices((x_val_uni, y_val_uni))\nval_univariate = val_univariate.batch(BATCH_SIZE).repeat()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"simple_lstm_model = tf.keras.models.Sequential([\n    tf.keras.layers.LSTM(8, input_shape=x_train_uni.shape[-2:]),\n    tf.keras.layers.Dense(1)\n])\n\nsimple_lstm_model.compile(optimizer='adam', loss='mae')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"EVALUATION_INTERVAL = 200\nEPOCHS = 100\n\nsimple_lstm_model.fit(train_univariate, epochs=EPOCHS,\n                      steps_per_epoch=EVALUATION_INTERVAL,\n                      validation_data=val_univariate, validation_steps=50)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for x, y in val_univariate.take(3):\n  plot = show_plot([x[0].numpy(), y[0].numpy(),\n                    simple_lstm_model.predict(x)[0]], 0, 'Simple LSTM model')\n  plot.show()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"losses_TF = simple_lstm_model.history.history['loss']\nplt.plot(range(len(losses_TF)), losses_TF)\n\n# losses_lstm = lstm_model.history.history['loss']\n# plt.figure(figsize=(12,4))\n# plt.xlabel(\"Epochs\")\n# plt.ylabel(\"Loss\")\n# plt.xticks(np.arange(0,21,1))\n# plt.plot(range(len(losses_lstm)),losses_lstm);","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# plot = show_plot([x[0].numpy(), y[0].numpy(),\n#                     simple_lstm_model.predict(x)[0]], 0, 'Simple LSTM model')\n# plot.show()\n\npred = simple_lstm_model.predict(x_val_uni)\nplt.plot()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}